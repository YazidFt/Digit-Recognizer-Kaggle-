{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.ndimage import convolve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import linear_model, datasets, metrics\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nudge_dataset(X, Y):\n",
    "    \"\"\"\n",
    "    This produces a dataset 5 times bigger than the original one,\n",
    "    by moving the 8x8 images in X around by 1px to left, right, down, up\n",
    "    \"\"\"\n",
    "    direction_vectors = [\n",
    "        [[0, 1, 0],\n",
    "         [0, 0, 0],\n",
    "         [0, 0, 0]],\n",
    "\n",
    "        [[0, 0, 0],\n",
    "         [1, 0, 0],\n",
    "         [0, 0, 0]],\n",
    "\n",
    "        [[0, 0, 0],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 0]],\n",
    "\n",
    "        [[0, 0, 0],\n",
    "         [0, 0, 0],\n",
    "         [0, 1, 0]]\n",
    "      ]\n",
    "\n",
    "    shift = lambda x, w: convolve(x.reshape((8, 8)), mode='constant',\n",
    "                                  weights=w).ravel()\n",
    "    X = np.concatenate([X] +\n",
    "                       [np.apply_along_axis(shift, 1, X, vector)\n",
    "                        for vector in direction_vectors])\n",
    "    Y = np.concatenate([Y for _ in range(5)], axis=0)\n",
    "    \n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "X  = data.drop('label', axis=1)\n",
    "target = data['label']\n",
    "\n",
    "#PCA\n",
    "n_components = 64\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized', whiten=True).fit(X)\n",
    "\n",
    "X_pca = pca.transform(X)\n",
    "test_pca = pca.transform(test)\n",
    "\n",
    "#OneHotEncoding\n",
    "target = pd.get_dummies(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BernoulliRBM\n",
    "X, Y = nudge_dataset(X_pca, target)\n",
    "\n",
    "#0-1 scaling\n",
    "X = (X - np.min(X, 0)) / (np.max(X, 0) + 0.0001)  \n",
    "test = (test_pca - np.min(test_pca, 0)) / (np.max(test_pca, 0) + 0.0001)\n",
    "\n",
    "#split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing Neural Network\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(units = 128, kernel_initializer = 'uniform', activation = 'tanh', input_dim = 64))\n",
    "classifier.add(Dense(units = 64, kernel_initializer = 'uniform', activation = 'tanh'))\n",
    "classifier.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'tanh'))\n",
    "classifier.add(Dense(units = 10, kernel_initializer = 'uniform', activation = 'softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "210000/210000 [==============================] - 9s 42us/step - loss: 1.9595 - acc: 0.3098\n",
      "Epoch 2/30\n",
      "210000/210000 [==============================] - 8s 37us/step - loss: 1.7388 - acc: 0.4170\n",
      "Epoch 3/30\n",
      "210000/210000 [==============================] - 8s 40us/step - loss: 1.5838 - acc: 0.4801\n",
      "Epoch 4/30\n",
      "210000/210000 [==============================] - 7s 36us/step - loss: 1.4278 - acc: 0.5366\n",
      "Epoch 5/30\n",
      "210000/210000 [==============================] - 7s 34us/step - loss: 1.2478 - acc: 0.6008\n",
      "Epoch 6/30\n",
      "210000/210000 [==============================] - 7s 36us/step - loss: 1.1044 - acc: 0.6500\n",
      "Epoch 7/30\n",
      "210000/210000 [==============================] - 7s 35us/step - loss: 0.9931 - acc: 0.6880\n",
      "Epoch 8/30\n",
      "210000/210000 [==============================] - 7s 36us/step - loss: 0.9049 - acc: 0.7173\n",
      "Epoch 9/30\n",
      "210000/210000 [==============================] - 8s 36us/step - loss: 0.8421 - acc: 0.7368\n",
      "Epoch 10/30\n",
      "210000/210000 [==============================] - 8s 40us/step - loss: 0.7931 - acc: 0.7533\n",
      "Epoch 11/30\n",
      "210000/210000 [==============================] - 8s 36us/step - loss: 0.7560 - acc: 0.7650\n",
      "Epoch 12/30\n",
      "210000/210000 [==============================] - 8s 37us/step - loss: 0.7307 - acc: 0.7728\n",
      "Epoch 13/30\n",
      "210000/210000 [==============================] - 8s 39us/step - loss: 0.7037 - acc: 0.7815\n",
      "Epoch 14/30\n",
      "210000/210000 [==============================] - 8s 36us/step - loss: 0.6844 - acc: 0.7872\n",
      "Epoch 15/30\n",
      "210000/210000 [==============================] - 8s 38us/step - loss: 0.6680 - acc: 0.7935\n",
      "Epoch 16/30\n",
      "210000/210000 [==============================] - 8s 38us/step - loss: 0.6557 - acc: 0.7962\n",
      "Epoch 17/30\n",
      "210000/210000 [==============================] - 9s 41us/step - loss: 0.6426 - acc: 0.8012\n",
      "Epoch 18/30\n",
      "210000/210000 [==============================] - 9s 42us/step - loss: 0.6303 - acc: 0.8055\n",
      "Epoch 19/30\n",
      "210000/210000 [==============================] - 8s 38us/step - loss: 0.6212 - acc: 0.8084\n",
      "Epoch 20/30\n",
      "210000/210000 [==============================] - 8s 38us/step - loss: 0.6136 - acc: 0.8113\n",
      "Epoch 21/30\n",
      "210000/210000 [==============================] - 8s 38us/step - loss: 0.6051 - acc: 0.8128\n",
      "Epoch 22/30\n",
      "210000/210000 [==============================] - 8s 38us/step - loss: 0.5948 - acc: 0.8163\n",
      "Epoch 23/30\n",
      "210000/210000 [==============================] - 8s 38us/step - loss: 0.5893 - acc: 0.8185\n",
      "Epoch 24/30\n",
      "210000/210000 [==============================] - 8s 38us/step - loss: 0.5834 - acc: 0.8197\n",
      "Epoch 25/30\n",
      "210000/210000 [==============================] - 8s 37us/step - loss: 0.5776 - acc: 0.8210\n",
      "Epoch 26/30\n",
      "210000/210000 [==============================] - 8s 38us/step - loss: 0.5699 - acc: 0.8238\n",
      "Epoch 27/30\n",
      "210000/210000 [==============================] - 8s 39us/step - loss: 0.5651 - acc: 0.8256\n",
      "Epoch 28/30\n",
      "210000/210000 [==============================] - 8s 37us/step - loss: 0.5602 - acc: 0.8272\n",
      "Epoch 29/30\n",
      "210000/210000 [==============================] - 8s 38us/step - loss: 0.5558 - acc: 0.8286\n",
      "Epoch 30/30\n",
      "210000/210000 [==============================] - 8s 37us/step - loss: 0.5486 - acc: 0.8308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2940fda0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling Neural Network\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "classifier.fit(X_train, Y_train, batch_size = 150, epochs = 30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicting the Test set\n",
    "y_pred = classifier.predict(test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_pred = y_pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Edit submission file\n",
    "li = [i+1 for i in range(len(test))]\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "        \"ImageId\": li,\n",
    "        \"Label\": y_pred\n",
    "     })\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
